<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Automatic sampling and analysis of YouTubeData</title>
    <meta charset="utf-8" />
    <meta name="author" content="Julian Kohne Johannes Breuer M. Rohangis Mohseni" />
    <meta name="date" content="2020-02-10" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
    <script src="libs/kePrint/kePrint.js"></script>
    <link rel="stylesheet" href="..\workshop.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Automatic sampling and analysis of YouTubeData
## Collecting data with the tuber package for R
### Julian Kohne<br />Johannes Breuer<br />M. Rohangis Mohseni
### 2020-02-10

---

layout: true



&lt;div class="my-footer"&gt;
  &lt;div style="float: left;"&gt;&lt;span&gt;Julian Kohne, Johannes Breuer, M. Rohangis Mohseni&lt;/span&gt;&lt;/div&gt;
  &lt;div style="float: right;"&gt;&lt;span&gt;GESIS, Cologne, Germany, 2020-02-10&lt;/span&gt;&lt;/div&gt;
  &lt;div style="text-align: center;"&gt;&lt;span&gt;Collecting data with the tuber package for R&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;style type="text/css"&gt;

pre {
  font-size: 10px
}
&lt;/style&gt;

---

# The `tuber` package

With the [`tuber` package](https://github.com/soodoku/tuber) you can access the *YouTube* API via `R`. The author of the package has written a short [vignette](https://cran.r-project.org/web/packages/tuber/vignettes/tuber-ex.html) to introduce some of its main functions.

Essentially, there are three main units of analysis for which you can access data with the `tuber` package:

- Videos (requires video IDs)
- Channels (requires channel IDs)
- Playlists (requires playlist IDs)

In this workshop, we will focus on individual **videos**.

---

# Setup


```r
install.packages("tuber")
install.packages("tidyverse")
library(tuber)
library(tidyverse)
options(stringsAsFactors = FALSE)
```

---

# Excursus: Keep your secrets secret

If you save your API credentials in an `R` script, anyone who has that script can potentially use your API key. This can affect your quota and might even cause costs for you.

&lt;img src="./pics/Google_API_key_alert.png" width="60%" style="display: block; margin: auto;" /&gt;
.center[
&lt;small&gt;&lt;small&gt;Source: https://twitter.com/sil_aarts/status/1187352423796477953&lt;/small&gt;&lt;/small&gt;
]

---

# Excursus: Keep your secrets secret

If you use `R` environment variables, your credentials won't be shared if you share your scripts. However, the information is still stored in a plain text file. You can run `usethis::edit_r_environ()` to see its contents.

One way of keeping your credentials safer is using the `keyring` package.


```r
install.packages("keyring")
library(keyring)
```

---

# Excursus: The `keyring` package

To store your credentials in an encrypted password-protected keyring you first need to create a new keyring.


```r
keyring_create("YouTube_API")
```

You will then be prompted to set a password for this keyring. As always: This password should ideally be easy to remember (for you) but hard to guess (for anyone else). You might want to write it down or store it in your password manager (if you use one). In the next step, you can store your *YouTube* app secret in the keyring.

```r
key_set("app_secret", 
        keyring ="YouTube_API")
```

When you are prompted to enter the password now you should enter your app secret. At the end of your script, you should lock the keyring again.

```r
keyring_lock("YouTube_API")
```

---

# Authenticate your app


```r
yt_oauth(app_id = "my_app_id", # paste your app ID here
         app_secret = "my_app_secret", # paste your app secret here
         token="")
```
or, if you stored your app secret in a keyring


```r
yt_oauth(app_id = "my_app_id",  # paste your app ID here
         app_secret = key_get("app_secret", keyring ="YouTube_API"),
         token="")
```

---

# Authentication

When running the `yt_oauth()` function there will be a prompt in the console asking you to save the access token in a file. If you select "Yes", `tuber` will create a local file (in your working directory) to cache [OAuth](https://en.wikipedia.org/wiki/OAuth) access credentials between `R` sessions. The token stored in this file will be valid for one hour (and can be automatically refreshed). Doing this makes sense if you know you will be running multiple `R` sessions that need your credentials (example: knitting an `RMarkdown` document). 

Similar to the app key and secret, you should never share the `.httr-oauth` file as it can be read in `R` using `read_RDS()` (the file contains a list list with all information needed for OAuth, including your app key and secret). Hence, it is usually ok (and safer) to select "No" when asked whether to store a local `.httr-oauth` file. 
---

# App permissions

After making your choice regarding the `.httr-oauth` file, a browser window should open, prompting you to log in with your *Google* account and to give permissions. *Google* will probably tell you that it is not safe to grant the app access to your account. However, since you created the app and presumably are its sole user, you can go ahead and trust yourself: Click "Extended", then on "Open App_Name", and grant the app the required permissions for your account. After that, you can close the browser and return to `R`.

---

# Channel statistics

You can find the channel ID by searching on the *YouTube* website and then choosing Type = Channel from the options in the filter menu. When you click on the channel you want in the search results you see the channel ID at the end of the URL (after "/channel/").




```r
hadley_stats &lt;- get_channel_stats("UCxOhDvtaoXDAB336AolWs3A")
```

```
## Channel Title: Hadley Wickham 
## No. of Views: 44515 
## No. of Subscribers: 2770 
## No. of Videos: 4
```

Note: If you want additional data on channels/video creators and their performance on *YouTube*, you can also check out the statistics provided by [*Socialblade*](https://socialblade.com/youtube/).

---

# Channel statistics

`get_channel_stats` returns a list which can then be wrangled into a dataframe (tibble).


```r
hadley_stats_df &lt;- hadley_stats$statistics %&gt;% 
  as_tibble() %&gt;% 
  mutate_at(vars(-hiddenSubscriberCount), as.numeric) %&gt;% 
  mutate(channel_id = hadley_stats$id,
         channel_name = hadley_stats$snippet$title,
         channel_published = hadley_stats$snippet$publishedAt)

hadley_stats_df
```

```
## # A tibble: 1 x 8
##   viewCount commentCount subscriberCount hiddenSubscribe~ videoCount
##       &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt; &lt;lgl&gt;                 &lt;dbl&gt;
## 1     44515            0            2770 FALSE                     4
## # ... with 3 more variables: channel_id &lt;chr&gt;, channel_name &lt;chr&gt;,
## #   channel_published &lt;chr&gt;
```

---

# Video statistics

The video IDs are the last 11 characters of the video URL. Example: https://www.youtube.com/watch?v=DcJFdCmN98s -&gt; ID = DcJFdCmN98s


```r
dayum_stats &lt;- get_stats("DcJFdCmN98s")

dayum_stats &lt;- dayum_stats %&gt;%
  as_tibble() %&gt;% 
  mutate_at(vars(-id), as.numeric)

dayum_stats
```

```
## # A tibble: 1 x 6
##   id          viewCount likeCount dislikeCount favoriteCount commentCount
##   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;
## 1 DcJFdCmN98s  38951178    529939         6786             0        48467
```

---

# Viewer comments

  There are two functions for collecting viewer comments for specific videos (both return a dataframe):

1. `get_comment_threads()` collects comments for a video. By default, it collects the 100 most recent comments. The number of comments can be changed with the argument `max_results` (needs to be &gt;= 20). If the number is &gt; 100, the function fetches all results (see `?get_comment_threads`). NB: This function does not collect replies to comments.

2. `get_all_comments()` collects all comments for a video, including replies to comments. NB: For some reason, this function only collects up to 5 replies per comment (the related [issue in the `tuber` GitHub repo](https://github.com/soodoku/tuber/issues/52) is still open). Depending on the number of comments for a video, running this function can take some time and might also deplete your API quota limit (more on that later).

---

# Other useful `tuber` functions

`get_video_details("video_id")`: list with metadata for the video (e.g., title, description, tags)

`get_all_channel_video_stats("channel_id")`: list with statistics for all videos in a channel

`get_playlists(filter=c(channel_id="channel_id"))`: list with playlists for a channel

`get_playlist_items(filter =c(playlist_id = "playlist_id"))`: dataframe with items from a playlist


For the full list of functions in the package see the [`tuber` reference manual](https://cran.r-project.org/web/packages/tuber/tuber.pdf).

---

# Searching video IDs

`tuber` provides the function `yt_search()` to use the API to search *YouTube*. However, using this function is extremely costly in terms of API queries. 

A single search query with the `yt_search()` function can easily exceed your daily quota limit as by default it returns data for all search results (you can check the number of results for a search query via the *YouTube* Data API Explorer). Hence, if you want to use the `yt_search()` function, we would strongly recommend setting the `get_all` argument to `FALSE`. This returns up to 50 results.


```r
yt_search(term = "tidyverse",
*         get_all = F)
```

---

# Searching video IDs

There are two alternatives to using the `yt_search()` function from the `tuber` package to search for *YouTube* video IDs:

1. Manual search via the *YouTube* website: This works well for a small number of videos but is not really feasible if you want a large number of video IDs for your analyses.

2. Web scraping: [Freelon (2018)](https://www.tandfonline.com/doi/abs/10.1080/10584609.2018.1477506?journalCode=upcp20) argues that researchers interested in social media and other internet data should know/learn how to scrape the web in what he calls the "post-API age". However, you should use this method with caution. The [robots.txt of *YouTube*](https://www.youtube.com/robots.txt) disallows the scraping of results pages. In practical terms, the (over)use of web scraping can, e.g., get your IP address blocked (at least temporarily). While an introduction to web scraping would be beyond the scope of this workshop (but there are many online tutorials available: [example](https://blog.rsquaredacademy.com/web-scraping/)). 

---

# Collecting comments for multiple videos

If you want to collect comments for multiple videos, you need a vector with all video IDs (in the code example below this vector is named `video_ids`). If you have that, you can use `map_df()` from the [`purrr` package](https://purrr.tidyverse.org/) to iterate over video IDs and create a combined dataframe (of course, you could also do this with a for-loop).


```r
comments &lt;- purrr::map_df(.x = video_ids,
                          .f = get_all_comments)
```

Important notice: Be aware that your daily API quota limit very likely won't allow you to collect all comments for videos with a high number of comments.

---

# YouTube API quota limits

Since January 11, 2019, the [*YouTube* Data API v3](https://developers.google.com/youtube/v3) has a limit of **10000 queries per day** (was 1M/day before that and even 50M/day until April 20, 2016).

The [*YouTube* Data API documentation](https://developers.google.com/youtube/v3/docs/) lists the costs of different API calls and allows you to test the API (without incurring any quota costs). There also is a [Quota Calculator](https://developers.google.com/youtube/v3/determine_quota_cost) for this API (note that the calculator does not list all types of API calls).

---

# YouTube API quota limits

You can monitor the quota use of your app via the [*Google* Cloud Developer Console](https://console.cloud.google.com). From the navigation menu, select *APIs &amp; Services*, then click on *YouTube Data API v3* in the list on the bottom of the page, and then on *Quotas* in the sidebar menu.

In order to be able to estimate the quota costs of a specific API call via the `tuber` package, you need to know the specific queries that the function you use produces. If you want to do this, you can print the function code (you can do this in R by executing the function name without () behind it: `get_stats`). You can then use the information from that code for the Quota Calculator or the API Explorer. The help file for the function of interest can also be helpful here (`?get_stats`).

---

# `tuber` function code


```r
get_stats
```

```
## function (video_id = NULL, ...) 
## {
##     if (!is.character(video_id)) 
##         stop("Must specify a video ID.")
##     querylist &lt;- list(part = "statistics", id = video_id)
##     raw_res &lt;- tuber_GET("videos", querylist, ...)
##     if (length(raw_res$items) == 0) {
##         warning("No statistics for this video are available.\n             Likely cause: Incorrect ID. \n")
##         return(list())
##     }
##     res &lt;- raw_res$items[[1]]
##     stat_res &lt;- res$statistics
##     c(id = res$id, stat_res)
## }
## &lt;bytecode: 0x0000000012a22d90&gt;
## &lt;environment: namespace:tuber&gt;
```

---

# Exemplary quota costs for `tuber` functions

&lt;table class="table" style="font-size: 14px; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Function &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Costs &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; API resource &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; API method &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; API parts &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; get_channel_stats('channel_id') &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Channels &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; list &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; statistics,snippet &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; get_stats('video_id') &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Videos &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; list &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; statistics &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; get_comment_threads('video_id', max_results = 100) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; CommentThreads &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; list &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; snippet &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

Be aware that the costs increase with the number of returned items/results.
For reference: `yt_search()` has a quota cost of 102 per page of results (with a default number of max. 50 results per page).

---

# How do I know the quota costs when I use `tuber`?

a) Estimate the costs based on costs for the specific query and the number of returned results. For example, if a video has 4000 comments (without replies) and you wish to collect all of them, you can estimate your cost as follows: 100 results per page = 40 pages &amp; cost per page for `get_comment_threads()` = 5 -&gt; overall cost = 200. In practice this is a bit more difficult to estimate as both the website of the video and `get_stats()` give you the comment count including replies.

b) Use the *Google* Cloud Console for your app to monitor your API quota use.

---

# Managing your API quota

The daily quota for your app resets at midnight Pacific Time. There are 3 ways of dealing with the quota limit:

1. Schedule your scripts. You can either do this manually (by splitting your data collection into batches and running the scripts on different days) or automatically (e.g., using the [taskScheduleR package](https://github.com/bnosac/taskscheduleR) for Windows or the [cronR package](https://github.com/bnosac/cronR) for Linux/Unix).

2. Request an increase through the Quotas menu in the developer console for your app mentioned previously. However, this will quite likely cost money and process is somewhat opaque and tedious as it requires you to fill out extended forms that were created for developers of actual apps.

3. Use multiple apps and/or accounts for your data collection. This requires some manual scheduling/planning. Keep in mind that there is a limited number of projects per account for the *Google* Cloud Platform. Note that - depending on your setup and use - this strategy might get your API quota, app(s), account(s) or IP address(es) suspended/blocked. 

---

# Manual quota limit management example

If you want to use `yt_search()` and want more than 50 search results, you can use the `page_token` argument to get more than one page of results.


```r
search_p1 &lt;- yt_search(term = "search term",
*                      simplify = F,
                       get_all = F)

page_token &lt;-search_p1$nextPageToken

search_p2 &lt;- yt_search(term = "search term",
                       get_all = F,
*                      page_token = page_token)

# turn search_p1 from a list into a dataframe
search_p1 &lt;- lapply(search_p1$items, function(x) unlist(x$snippet))
search_p1 &lt;- plyr::ldply(search_p1, rbind)

search_results &lt;- bind_rows(search_p1, search_p2)
```

---

class: center, middle

# [Exercise](https://jobreu.github.io/tidyverse-workshop-gesis-2019/exercises/A5_DataWrangling1_exercises_question.html) time üèãÔ∏è‚Äç‚ôÄÔ∏èüí™üèÉüö¥

## [Solutions](https://jobreu.github.io/tidyverse-workshop-gesis-2019/solutions/A5_DataWrangling1_exercises_solution.html)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
