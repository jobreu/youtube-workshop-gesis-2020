<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Automatic sampling and analysis of YouTubeData</title>
    <meta charset="utf-8" />
    <meta name="author" content="Julian Kohne Johannes Breuer M. Rohangis Mohseni" />
    <meta name="date" content="2020-02-11" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="..\workshop.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Automatic sampling and analysis of YouTubeData
## Basic text analysis of user comments
### Julian Kohne<br />Johannes Breuer<br />M. Rohangis Mohseni
### 2020-02-11

---

layout: true



&lt;div class="my-footer"&gt;
  &lt;div style="float: left;"&gt;&lt;span&gt;Julian Kohne, Johannes Breuer, M. Rohangis Mohseni&lt;/span&gt;&lt;/div&gt;
  &lt;div style="float: right;"&gt;&lt;span&gt;GESIS, Cologne, Germany, 2020-02-11&lt;/span&gt;&lt;/div&gt;
  &lt;div style="text-align: center;"&gt;&lt;span&gt;Basic text analysis of user comments&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;style type="text/css"&gt;

pre {
  font-size: 10px
}
&lt;/style&gt;

---

# Required libraries for this session


```r
library(tidyverse)
library(lubridate)
library(tuber)
library(quanteda)
library(devtools)
install_github("dill/emoGG")
install_github("hadley/emo")
```

---

# Collect &amp; parse the data


```r
Comments &lt;- get_all_comments(c(video_id="DcJFdCmN98s")) # takes a while
source("yt_parse.R") # yt_parse.R needs to be in the same directory
FormattedComments &lt;- yt_parse(Comments) # will take a while
```



---

# Comments over time: Data wrangling ü§†

We want to plot the development of the number of comments per week over time and show until when 50%, 75%, 90%, and 99% of the comments had been posted. This requires some data wrangling.


```r
FormattedComments &lt;- FormattedComments %&gt;% 
  arrange(Published) %&gt;% 
  mutate(date = date(Published),
         week = floor_date(date, 
                           unit = "week",
                           week_start = getOption("lubridate.week.start", 1)),
         counter = 1)

weekly_comments &lt;- FormattedComments %&gt;% 
  group_by(week) %&gt;% 
  summarize(count = n()) %&gt;% 
  mutate(cumulative_count = cumsum(count))

PercTimes &lt;- round(quantile(cumsum(FormattedComments$counter), 
                            probs = c(0.5, 0.75, 0.9, 0.99)))
```

---

# Comments over time: Plot


```r
weekly_comments %&gt;% 
  ggplot(aes(x = week, y = count)) +
  geom_bar(stat = "identity") +
  scale_x_date(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0,7000)) +
  labs(title = "Number of comments over time",
  subtitle = "Schmoyoho - OH MY DAYUM ft. Daym Drops
       \nhttps://www.youtube.com/watch?v=DcJFdCmN98s",
  x = "Week",
  y = "# of comments") +
  geom_vline(xintercept = FormattedComments$week[PercTimes],linetype = "dashed", colour = "red") +
  geom_text(aes(x = FormattedComments$week[PercTimes][1], label = "50%", y = 3500),
            colour="red", angle=90, vjust = 1.2) +
  geom_text(aes(x = FormattedComments$week[PercTimes][2], label = "75%", y = 3500),
            colour="red", angle=90, vjust = 1.2) +
  geom_text(aes(x = FormattedComments$week[PercTimes][3], label = "90%", y = 3500),
            colour="red", angle=90, vjust = 1.2) +
  geom_text(aes(x = FormattedComments$week[PercTimes][4], label = "99%", y = 3500), 
            colour="red", angle=90, vjust = 1.2)
```

---

# Number of comments over time: Plot

&lt;img src="B1_Basic_Text_Analysis_files/figure-html/comments over time plot-1.png" width="700px" height="500px" style="display: block; margin: auto;" /&gt;
---

# Text mining

In this session, we will discuss some basic exploratory analyses of *YouTube* user comments. We will explore the use of words as well as the use of emojis.

An introduction to text mining is beyond the scope of this workshop, but there are many great introductions available (for free) online. For example:

- [Text Mining in R](https://www.tidytextmining.com/) by Julia Silge &amp; David Robinson: A tidy(verse) approach

- [Tutorials for the package `quanteda`](https://tutorials.quanteda.io/)

- [Text mining for humanists and social scientists in R](https://tm4ss.github.io/docs/) by Andreas Niekler &amp; Gregor Wiedemann

- [Automatisierte Inhaltsanalyse mit `R`](http://inhaltsanalyse-mit-r.de/) by Cornelius Puschmann

In the following, we will very briefly introduce some key terms and steps in text mining, and then go through some examples of exploring *YouTube* comments (text + emojis).

---

# Popular text mining packages

- [tm](http://tm.r-forge.r-project.org/): the first comprehensive text mining package for R

- [tidytext](https://juliasilge.github.io/tidytext/): tidyverse tools &amp; tidy data principles

- [**quanteda**](https://quanteda.io/): very powerful text mining package with extensive documentation

---

# Text as data (in a üå∞)

**Document** = collection of strings (+ metadata about the documents)

**Corpus** = collection of documents

**Token** = part of a text that is a meaningful unit of analysis (often individual words)

**Vocabulary** = list of all distinct words form a corpus

**Document-term matrix (DTM)** or **Document-feature matrix (DFM)** = matrix with *n* = # of documents rows and *m* = size of vocabulary columns where each cell contains the count of a particular word for a particular document

---

# Preprocessing (in a üå∞)

For our examples in this session, we will go through the following preprocessing steps:

1. **Basic string operations**: 
  - Transforming to lower case
  - Detecting and removing certain patterns in strings (e.g., punctuation, numbers or URLs)
  
2. **Tokenization**: Splitting up strings into words (could also be combinations of multiple words: n-grams)

3. **Stop word removal**: Stopwords are very frequent words that appear in almost all texts (e.g., "a","but","it", "the")

**NB**: There are many other preprocessing options that we will not use for our examples, such as [stemming](https://en.wikipedia.org/wiki/Stemming), [lemmatization](https://en.wikipedia.org/wiki/Lemmatisation) or natural language processing pipelines (e.g., to detect and select specific word types, such as nouns and adjectives). Keep in mind that the choice and order of these preprocessing steps is important and should be informed by your research question.

---

# Tokenization

Before we tokenize the comments, we want to remove newline commands from the strings


```r
FormattedComments &lt;- FormattedComments %&gt;% 
  mutate(TextEmojiDeleted = str_replace_all(TextEmojiDeleted,
                                            pattern = "\\\n",
                                            replacement = " "))
```

Now we can tokenize the comments and remove punctuation, symbols, numbers, and URLs.


```r
toks &lt;- FormattedComments %&gt;% 
  pull(TextEmojiDeleted) %&gt;% 
  char_tolower() %&gt;% 
  tokens(remove_numbers = TRUE,
               remove_punct = TRUE,
               remove_separators = TRUE,
               remove_symbols = TRUE,
               remove_hyphens = TRUE,
               remove_url = TRUE)
```

---

# Document-feature matrix

Now we create a document-feature matrix (DFM) and remove stopwords.


```r
commentsDfm &lt;- dfm(toks, 
                   remove = quanteda::stopwords("english"))
```

---

# Most frequent words


```r
TermFreq &lt;- textstat_frequency(commentsDfm)
TermFreq[10:20, ]
```

```
##    feature frequency rank docfreq group
## 10    just      1949   10    1849   all
## 11      oh      1796   11    1333   all
## 12     now      1769   12    1730   all
## 13     get      1756   13    1695   all
## 14    good      1660   14    1609   all
## 15    best      1646   15    1623   all
## 16     one      1390   16    1340   all
## 17       d      1171   17    1123   all
## 18     guy      1166   18    1129   all
## 19      xd      1153   19    1128   all
## 20    damn      1137   20     821   all
```

---

# Removing tokens

There seem to be ASCII emojis among the most frequent tokens. We might want to remove these and/or other tokens if we consider them irrelevant for our analyses.


```r
custom_stopwords &lt;- c("video","d","xd", "youtube")
commentsDfm &lt;- dfm(toks, remove = c(quanteda::stopwords("english"),
                                    custom_stopwords))
TermFreq &lt;- textstat_frequency(commentsDfm)
```

For more options for selecting or removing tokens, see the [quanteda documentation](https://tutorials.quanteda.io/basic-operations/tokens/tokens_select/).

---

# Plot most frequent words


```r
head(TermFreq, n = 20) %&gt;% 
ggplot(aes(x = reorder(feature, -frequency), y = frequency)) +
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 90,
                                   hjust = 1,
                                   vjust = 0.3)) +
  labs(title = "Most frequent words in comments",
       subtitle = "Schmoyoho - OH MY DAYUM ft. Daym Drops
       \nhttps://www.youtube.com/watch?v=DcJFdCmN98s",
       x = "",
       y = "Frequency") +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0,10000)) +
  theme(panel.grid.major.x = element_blank())
```

---

# Plot most frequent words

&lt;img src="B1_Basic_Text_Analysis_files/figure-html/word freq plot-1.png" width="700px" height="500px" style="display: block; margin: auto;" /&gt;

---

# Plot docfreq


```r
head(TermFreq, n = 20) %&gt;% 
ggplot(aes(x = reorder(feature, -docfreq), y = docfreq)) +
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 90,
                                   hjust = 1,
                                   vjust = 0.3)) +
  labs(title = "Words that appear in the highest number of comments",
       subtitle = "Schmoyoho - OH MY DAYUM ft. Daym Drops
       \nhttps://www.youtube.com/watch?v=DcJFdCmN98s",
       x = "",
       y = "# of comments") +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0,6500)) +
  theme(panel.grid.major.x = element_blank())
```

---

# Plot docfreq

&lt;img src="B1_Basic_Text_Analysis_files/figure-html/docfreq plot-1.png" width="700px" height="500px" style="display: block; margin: auto;" /&gt;

---

# Emojis

In most of the research studying user-generated text from social media emojis have, so far, been largely ignored. However, emojis convey emotions and meaning, and can, thus, provide additional information or context when working with textual data. 

In the following, we will do some exploratory analysis of emoji frequencies in *YouTube* comments. Before we can start, we first need to do some data cleaning again, then tokenize the emojis as some comments include more than one emoji, and create an emoji DFM.


```r
emoji_toks &lt;- FormattedComments %&gt;% 
  mutate_at(c("Emoji"), list(~na_if(., "NA"))) %&gt;% # define missings
  mutate (Emoji = str_trim(Emoji)) %&gt;% # remove spaces
  filter(!is.na(Emoji)) %&gt;% # only keep comments with emojis
  pull(Emoji) %&gt;% # pull out column cotaining emoji labels
  tokens() # tokenize emoji labels

EmojiDfm &lt;- dfm(emoji_toks) # create DFM for emojis
```

---

# Most frequent emojis


```r
EmojiFreq &lt;- textstat_frequency(EmojiDfm)
head(EmojiFreq, n = 10)
```

```
##                            feature frequency rank docfreq group
## 1         emoji_facewithtearsofjoy       732    1     261   all
## 2                  emoji_hamburger       343    2      44   all
## 3                emoji_frenchfries       177    3      19   all
## 4  emoji_smilingfacewithsunglasses       139    4      11   all
## 5                emoji_smilingface       116    5       7   all
## 6                       emoji_fire        63    6      20   all
## 7           emoji_loudlycryingface        42    7      24   all
## 8  emoji_smilingfacewithheart-eyes        36    8      16   all
## 9  emoji_rollingonthefloorlaughing        33    9      19   all
## 10                  emoji_redheart        31   10      22   all
```

---

# Plot most frequent emojis


```r
head(EmojiFreq, n = 10) %&gt;% 
ggplot(aes(x = reorder(feature, -frequency), y = frequency)) +
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 90,
                                   hjust = 1,
                                   vjust = 0.3)) +
  labs(title = "Most frequent emojis in comments",
       subtitle = "Schmoyoho - OH MY DAYUM ft. Daym Drops
       \nhttps://www.youtube.com/watch?v=DcJFdCmN98s",
       x = "",
       y = "Frequency") +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0,800)) +
  theme(panel.grid.major.x = element_blank())
```

Note: Similar to what we did for the comment text before we could replace `frequency` with `docfreq` in the above code to create a plot with the emojis that appear in the highest number of comments.

---

# Plot most frequent emojis

&lt;img src="B1_Basic_Text_Analysis_files/figure-html/lame emoji bar plot-1.png" width="700px" height="500px" style="display: block; margin: auto;" /&gt;

---

# üòé emoji frequency plot: Preparation 1

The previous emoji frequency plot was a bit üò™. To make things prettier we can use the actual emojis instead of the text labels in our plot. Doing this takes a bit of preparation...&lt;sup&gt;1&lt;/sup&gt;

As a first step, we need an emoji lookup table in which the values in the name column have the same format as the labels in the feature column of our `EmojiFreq` object.


```r
emoji_lookup &lt;- jis %&gt;% 
  select(runes, name) %&gt;% 
  mutate(runes = str_to_lower(runes),
         name = str_to_lower(name)) %&gt;% 
  mutate(name = str_replace_all(name, " ", "")) %&gt;% 
  mutate(name = paste0("emoji_", name))
```

.footnote[
[1] For an alternative approach to using emojis in `ggplot2` see this [blog post by Emil Hvitfeldt](https://www.hvitfeldt.me/blog/real-emojis-in-ggplot2/)
]

---

# üòé emoji frequency plot: Preparation 2

The second step of preparation for the nicer emoji frequency plot is creating mappings of emojis to data points so that we can use emojis instead of points in a scatter plot.


```r
top_emojis &lt;- 1:10

for(i in top_emojis){
  name &lt;- paste0("mapping", i)
  assign(name,
         do.call(geom_emoji,list(data = EmojiFreq[i,],
                                 emoji = strsplit(tolower(emoji_lookup$runes[emoji_lookup$name == as.character(EmojiFreq[i,]$feature)])," ")[[1]][1])))
}
```

---

# üòé emoji frequency plot

.small[

```r
head(EmojiFreq, n = 10) %&gt;% 
ggplot(aes(x = reorder(feature, -frequency), y = frequency)) +
  geom_bar(stat="identity",
           color = "black",
           fill = "#FFCC4D") + 
  geom_point() +
  labs(title = "Most frequent emojis in comments",
       subtitle = "Schmoyoho - OH MY DAYUM ft. Daym Drops
       \nhttps://www.youtube.com/watch?v=DcJFdCmN98s",
       x = "",
       y = "Frequency") +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0,800)) +
  theme(panel.grid.major.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) +
  mapping1 +
  mapping2 +
  mapping3 +
  mapping4 +
  mapping5 +
  mapping6 +
  mapping7 +
  mapping8 +
  mapping9 +
  mapping10
```
]

---

# üòé emoji frequency plot

&lt;img src="B1_Basic_Text_Analysis_files/figure-html/cool emoji bar plot-1.png" width="700px" height="500px" style="display: block; margin: auto;" /&gt;

---

class: center, middle

# [Exercise](https://jobreu.github.io/tidyverse-workshop-gesis-2019/exercises/A5_DataWrangling1_exercises_question.html) time üèãÔ∏è‚Äç‚ôÄÔ∏èüí™üèÉüö¥

## [Solutions](https://jobreu.github.io/tidyverse-workshop-gesis-2019/solutions/A5_DataWrangling1_exercises_solution.html)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
