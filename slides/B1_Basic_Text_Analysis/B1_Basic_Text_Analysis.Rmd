---
title: "Automatic sampling and Analysis of YouTube Data"
subtitle: "Basic Text Analysis of User Comments"
author: "Julian Kohne<br />Johannes Breuer<br />M. Rohangis Mohseni"
date: "2020-02-11"
location: "GESIS, Cologne, Germany"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["default", "default-fonts", "../workshop.css"]
    nature:
      highlightStyle: "github"
      highlightLines: true
      countIncrementalSlides: false
---
layout: true

```{r setup, include = F}
if (!require(easypackages)) install.packages("easypackages")
library(easypackages)

packages("knitr",
         "rmarkdown",
         "tidyverse",
         "lubridate",
         "kableExtra",
         "hadley/emo",
         "tuber",
         "dill/emoGG",
         "quanteda",
         prompt = F)

options(htmltools.dir.version = FALSE, stringsAsFactors = F)

opts_chunk$set(echo = TRUE, fig.align = "center")

```

<div class="my-footer">
  <div style="float: left;"><span>`r gsub("<br />", ", ", gsub("<br /><br />|<a.+$", "", metadata$author))`</span></div>
  <div style="float: right;"><span>`r metadata$location`, `r metadata$date`</span></div>
  <div style="text-align: center;"><span>`r gsub(".+<br />", " ", metadata$subtitle)`</span></div>
</div>

---

# Required Libraries for This Session

```{r libraries, eval=F}
library(tidyverse)
library(lubridate)
library(tuber)
library(quanteda)
library(devtools)
install_github("dill/emoGG")
install_github("hadley/emo")
```

---

# Collect & Parse the Data

Note: To save time and your *YouTube* API quota limit we suggest that you don't "code along" in this session

```{r get & parse comments, eval=F}
Comments <- get_all_comments(c(video_id="DcJFdCmN98s")) # takes a while
source("yt_parse.R") # yt_parse.R needs to be in the working directory here
FormattedComments <- yt_parse(Comments) # will take a while
```

```{r load data, echo=F}
load("../../data/DayumParsedCommentsUTF8.Rdata")
```

---

# Comments Over Time: Data Wrangling `r ji("cowboy")`

First, we want to plot the development of the number of comments per week over time and show until when 50%, 75%, 90%, and 99% of the comments had been posted. This requires some data wrangling.

```{r wrangle comments over time}
FormattedComments <- FormattedComments %>% 
  arrange(Published) %>% 
  mutate(date = date(Published),
         week = floor_date(date, 
                           unit = "week",
                           week_start = getOption("lubridate.week.start", 1)),
         counter = 1)

weekly_comments <- FormattedComments %>% 
  group_by(week) %>% 
  summarize(count = n()) %>% 
  mutate(cumulative_count = cumsum(count))

PercTimes <- round(quantile(cumsum(FormattedComments$counter), 
                            probs = c(0.5, 0.75, 0.9, 0.99)))
```

---

# Comments Over Time: Plot

```{r comments over time plot code, eval=F}
weekly_comments %>% 
  ggplot(aes(x = week, y = count)) +
  geom_bar(stat = "identity") +
  scale_x_date(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0,7000)) +
  labs(title = "Number of comments over time",
  subtitle = "Schmoyoho - OH MY DAYUM ft. Daym Drops
       \nhttps://www.youtube.com/watch?v=DcJFdCmN98s",
  x = "Week",
  y = "# of comments") +
  geom_vline(xintercept = FormattedComments$week[PercTimes],linetype = "dashed", colour = "red") +
  geom_text(aes(x = FormattedComments$week[PercTimes][1], label = "50%", y = 3500),
            colour="red", angle=90, vjust = 1.2) +
  geom_text(aes(x = FormattedComments$week[PercTimes][2], label = "75%", y = 3500),
            colour="red", angle=90, vjust = 1.2) +
  geom_text(aes(x = FormattedComments$week[PercTimes][3], label = "90%", y = 3500),
            colour="red", angle=90, vjust = 1.2) +
  geom_text(aes(x = FormattedComments$week[PercTimes][4], label = "99%", y = 3500), 
            colour="red", angle=90, vjust = 1.2)
```

---

# Number of Comments Over Time: Plot

```{r comments over time plot, echo=F, dpi=300, out.width="700px", out.height="500px"}
PercTimes <- round(quantile(cumsum(FormattedComments$counter), 
                            probs = c(0.5, 0.75, 0.9, 0.99)))

weekly_comments %>% 
  ggplot(aes(x = week, y = count)) +
  geom_bar(stat = "identity") +
  scale_x_date(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0,7000)) +
  labs(title = "Number of comments over time",
  subtitle = "Schmoyoho - OH MY DAYUM ft. Daym Drops
       \nhttps://www.youtube.com/watch?v=DcJFdCmN98s",
  x = "Week",
  y = "# of comments") +
  geom_vline(xintercept = FormattedComments$week[PercTimes],linetype = "dashed", colour = "red") +
  geom_text(aes(x = FormattedComments$week[PercTimes][1], label = "50%", y = 3500),
            colour="red", angle=90, vjust = 1.2) +
  geom_text(aes(x = FormattedComments$week[PercTimes][2], label = "75%", y = 3500),
            colour="red", angle=90, vjust = 1.2) +
  geom_text(aes(x = FormattedComments$week[PercTimes][3], label = "90%", y = 3500),
            colour="red", angle=90, vjust = 1.2) +
  geom_text(aes(x = FormattedComments$week[PercTimes][4], label = "99%", y = 3500), 
            colour="red", angle=90, vjust = 1.2)
```
---

# Text Mining

In this session, we will discuss some basic exploratory analyses of *YouTube* user comments. We will explore the use of words as well as the use of emojis.

An introduction to text mining is beyond the scope of this workshop, but there are many great introductions available (for free) online. For example:

- [Text Mining in R](https://www.tidytextmining.com/) by Julia Silge & David Robinson: A tidy(verse) approach

- [Tutorials for the package `quanteda`](https://tutorials.quanteda.io/)

- [Text mining for humanists and social scientists in R](https://tm4ss.github.io/docs/) by Andreas Niekler & Gregor Wiedemann

- [Text Mining in R](https://www.kirenz.com/post/2019-09-16-r-text-mining/) by Jan Kirenz

- [Automatisierte Inhaltsanalyse mit `R`](http://inhaltsanalyse-mit-r.de/) by Cornelius Puschmann

In the following, we will very briefly introduce some key terms and steps in text mining, and then go through some examples of exploring *YouTube* comments (text + emojis).

---

# Popular Text Mining Packages

- [tm](http://tm.r-forge.r-project.org/): the first comprehensive text mining package for R

- [tidytext](https://juliasilge.github.io/tidytext/): tidyverse tools & tidy data principles

- [**quanteda**](https://quanteda.io/): very powerful text mining package with extensive documentation

---

# Text as Data (in a `r ji("chestnut")`)

**Document** = collection of strings (+ metadata about the documents)

**Corpus** = collection of documents

**Token** = part of a text that is a meaningful unit of analysis (often individual words)

**Vocabulary** = list of all distinct words form a corpus

**Document-term matrix (DTM)** or **Document-feature matrix (DFM)** = matrix with *n* = # of documents rows and *m* = size of vocabulary columns where each cell contains the count of a particular word for a particular document

---

# Preprocessing (in a `r ji("chestnut")`)

For our examples in this session, we will go through the following preprocessing steps:

1. **Basic string operations**: 
  - Transforming to lower case
  - Detecting and removing certain patterns in strings (e.g., punctuation, numbers or URLs)
  
2. **Tokenization**: Splitting up strings into words (could also be combinations of multiple words: n-grams)

3. **Stop word removal**: Stopwords are very frequent words that appear in almost all texts (e.g., "a","but","it", "the")

**NB**: There are many other preprocessing options that we will not use for our examples, such as [stemming](https://en.wikipedia.org/wiki/Stemming), [lemmatization](https://en.wikipedia.org/wiki/Lemmatisation) or natural language processing pipelines (e.g., to detect and select specific word types, such as nouns and adjectives). Keep in mind that the choice and order of these preprocessing steps is important and should be informed by your research question.

---

# Tokenization

Before we tokenize the comments, we want to remove newline commands from the strings.

```{r remove newline}
FormattedComments <- FormattedComments %>% 
  mutate(TextEmojiDeleted = str_replace_all(TextEmojiDeleted,
                                            pattern = "\\\n",
                                            replacement = " "))
```

Now we can tokenize the comments and remove punctuation, symbols, numbers, and URLs.

```{r tokenization}
toks <- FormattedComments %>% 
  pull(TextEmojiDeleted) %>% 
  char_tolower() %>% 
  tokens(remove_numbers = TRUE,
               remove_punct = TRUE,
               remove_separators = TRUE,
               remove_symbols = TRUE,
               remove_hyphens = TRUE,
               remove_url = TRUE)
```

---

# Document-Feature Matrix

With the tokens we can create a document-feature matrix (DFM) and remove stopwords.

```{r dfm}
commentsDfm <- dfm(toks, 
                   remove = quanteda::stopwords("english"))
```

---

# Most Frequent Words

```{r word freq}
TermFreq <- textstat_frequency(commentsDfm)
TermFreq[10:20, ]
```

---

# Removing Tokens

There seem to be ASCII emojis among the most frequent tokens. We might want to remove these and/or other tokens if we consider them irrelevant for our analyses.

```{r custom stops}
custom_stopwords <- c("video","d","xd", "youtube")
commentsDfm <- dfm(toks, remove = c(quanteda::stopwords("english"),
                                    custom_stopwords))
TermFreq <- textstat_frequency(commentsDfm)
```

For more options for selecting or removing tokens, see the [quanteda documentation](https://tutorials.quanteda.io/basic-operations/tokens/tokens_select/).

---

# Plot Most Frequent Words (1)

```{r code word freq plot, eval=F}
head(TermFreq, n = 20) %>% 
ggplot(aes(x = reorder(feature, -frequency), y = frequency)) +
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 90,
                                   hjust = 1,
                                   vjust = 0.3)) +
  labs(title = "Most frequent words in comments",
       subtitle = "Schmoyoho - OH MY DAYUM ft. Daym Drops
       \nhttps://www.youtube.com/watch?v=DcJFdCmN98s",
       x = "",
       y = "Frequency") +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0,10000)) +
  theme(panel.grid.major.x = element_blank())
```

---

# Plot Most Frequent Words (2)

```{r word freq plot, echo=F, dpi=300, out.width="700px", out.height="500px"}
head(TermFreq, n = 20) %>% 
ggplot(aes(x = reorder(feature, -frequency), y = frequency)) +
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 90,
                                   hjust = 1,
                                   vjust = 0.3)) +
  labs(title = "Most frequent words in comments",
       subtitle = "Schmoyoho - OH MY DAYUM ft. Daym Drops
       \nhttps://www.youtube.com/watch?v=DcJFdCmN98s",
       x = "",
       y = "Frequency") +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0,10000)) +
  theme(panel.grid.major.x = element_blank())
```

---

# Plot Docfreq (1)

Instead of the raw frequency of words we can also look at the number of comments that a particular word appears in. This metric takes into account that words might be used multiple times in the same comment.

```{r code docfreq plot, eval=F}
head(TermFreq, n = 20) %>% 
ggplot(aes(x = reorder(feature, -docfreq), y = docfreq)) +
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 90,
                                   hjust = 1,
                                   vjust = 0.3)) +
  labs(title = "Words that appear in the highest number of comments",
       subtitle = "Schmoyoho - OH MY DAYUM ft. Daym Drops
       \nhttps://www.youtube.com/watch?v=DcJFdCmN98s",
       x = "",
       y = "# of comments") +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0,6500)) +
  theme(panel.grid.major.x = element_blank())
```

---

# Plot Docfreq (2)

```{r docfreq plot, echo=F, dpi=300, out.width="700px", out.height="500px"}
head(TermFreq, n = 20) %>% 
ggplot(aes(x = reorder(feature, -docfreq), y = docfreq)) +
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 90,
                                   hjust = 1,
                                   vjust = 0.3)) +
  labs(title = "Words that appear in the highest number of comments",
       subtitle = "Schmoyoho - OH MY DAYUM ft. Daym Drops
       \nhttps://www.youtube.com/watch?v=DcJFdCmN98s",
       x = "",
       y = "# of comments") +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0,6500)) +
  theme(panel.grid.major.x = element_blank())
```

---

# Emojis

In most of the research studying user-generated text from social media, emojis have, so far, been largely ignored. However, emojis convey emotions and meaning, and can, thus, provide additional information or context when working with textual data. 

In the following, we will do some exploratory analysis of emoji frequencies in *YouTube* comments. Before we can start, we first need to do some data cleaning again, then tokenize the emojis as some comments include more than one emoji, and create an emoji DFM.

```{r wrangle emoji data}
emoji_toks <- FormattedComments %>% 
  mutate_at(c("Emoji"), list(~na_if(., "NA"))) %>% # define missings
  mutate (Emoji = str_trim(Emoji)) %>% # remove spaces
  filter(!is.na(Emoji)) %>% # only keep comments with emojis
  pull(Emoji) %>% # pull out column cotaining emoji labels
  tokens() # tokenize emoji labels

EmojiDfm <- dfm(emoji_toks) # create DFM for emojis
```

---

# Most Frequent Emojis

```{r emoji freq}
EmojiFreq <- textstat_frequency(EmojiDfm)
head(EmojiFreq, n = 10)
```

---

# Plot Most Frequent Emojis (1)

```{r lame emoji bar plot code, eval=F}
head(EmojiFreq, n = 10) %>% 
ggplot(aes(x = reorder(feature, -frequency), y = frequency)) +
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 90,
                                   hjust = 1,
                                   vjust = 0.3)) +
  labs(title = "Most frequent emojis in comments",
       subtitle = "Schmoyoho - OH MY DAYUM ft. Daym Drops
       \nhttps://www.youtube.com/watch?v=DcJFdCmN98s",
       x = "",
       y = "Frequency") +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0,800)) +
  theme(panel.grid.major.x = element_blank())
```

Note: Similar to what we did for the comment text before we could replace `frequency` with `docfreq` in the above code to create a plot with the emojis that appear in the highest number of comments.

---

# Plot Most Frequent Emojis (2)

```{r lame emoji bar plot, echo=F, dpi=300, out.width="700px", out.height="500px"}
head(EmojiFreq, n = 10) %>% 
ggplot(aes(x = reorder(feature, -frequency), y = frequency)) +
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 90,
                                   hjust = 1,
                                   vjust = 0.3)) +
  labs(title = "Most frequent emojis in comments",
       subtitle = "Schmoyoho - OH MY DAYUM ft. Daym Drops
       \nhttps://www.youtube.com/watch?v=DcJFdCmN98s",
       x = "",
       y = "Frequency") +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0,800)) +
  theme(panel.grid.major.x = element_blank())
```

---

# `r ji("sunglasses")` Emoji Frequency Plot: Preparation (1)

The previous emoji frequency plot was a bit `r ji("sleepy_face")`. To make things prettier, we can use the actual emojis instead of the text labels in our plot. Doing this takes a bit of preparation...<sup>1</sup>

As a first step, we need an emoji lookup table in which the values in the name column have the same format as the labels in the feature column of our `EmojiFreq` object.

```{r emoji lookup table}
emoji_lookup <- jis %>% 
  select(runes, name) %>% 
  mutate(runes = str_to_lower(runes),
         name = str_to_lower(name)) %>% 
  mutate(name = str_replace_all(name, " ", "")) %>% 
  mutate(name = paste0("emoji_", name))
```

.footnote[
[1] For an alternative approach to using emojis in `ggplot2` see this [blog post by Emil Hvitfeldt](https://www.hvitfeldt.me/blog/real-emojis-in-ggplot2/).
]

---

# `r ji("sunglasses")` Emoji Frequency Plot: Preparation (2)

The second step of preparation for the nicer emoji frequency plot is creating mappings of emojis to data points so that we can use emojis instead of points in a scatter plot.<sup>1</sup>

```{r emoji mappings}
top_emojis <- 1:10

for(i in top_emojis){
  name <- paste0("mapping", i)
  assign(name,
         do.call(geom_emoji,list(data = EmojiFreq[i,],
                                 emoji = gsub("^0{2}","", strsplit(tolower(emoji_lookup$runes[emoji_lookup$name == as.character(EmojiFreq[i,]$feature)])," ")[[1]][1]))))
}
```

.footnote[
[1] Please note that this code has not been tested systematically. We only used it with a few videos. Depending on which emojis are the most frequent for the video you look at, this might not work because (a) one of the emojis is not included in the emoji lookup table (which uses the `jis` data frame from the [`emo` package](https://github.com/hadley/emo)) or (b) the content in the `runes` column does not match the format/code that the `emoji` argument in the `geom_emoji` function from the [`emoGG` package](https://github.com/dill/emoGG) expects.
]

---

# `r ji("sunglasses")` Emoji Frequency Plot (1)

.small[
```{r cool emoji bar plot code, eval=F}
head(EmojiFreq, n = 10) %>% 
ggplot(aes(x = reorder(feature, -frequency), y = frequency)) +
  geom_bar(stat="identity",
           color = "black",
           fill = "#FFCC4D") + 
  geom_point() +
  labs(title = "Most frequent emojis in comments",
       subtitle = "Schmoyoho - OH MY DAYUM ft. Daym Drops
       \nhttps://www.youtube.com/watch?v=DcJFdCmN98s",
       x = "",
       y = "Frequency") +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0,800)) +
  theme(panel.grid.major.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) +
  mapping1 +
  mapping2 +
  mapping3 +
  mapping4 +
  mapping5 +
  mapping6 +
  mapping7 +
  mapping8 +
  mapping9 +
  mapping10
```
]

---

# `r ji("sunglasses")` Emoji Frequency Plot (2)

```{r cool emoji bar plot, echo=F, dpi=300, out.width="700px", out.height="500px"}
head(EmojiFreq, n = 10) %>% 
ggplot(aes(x = reorder(feature, -frequency), y = frequency)) +
  geom_bar(stat="identity",
           color = "black",
           fill = "#FFCC4D") + 
  geom_point() +
  labs(title = "Most frequent emojis in comments",
       subtitle = "Schmoyoho - OH MY DAYUM ft. Daym Drops
       \nhttps://www.youtube.com/watch?v=DcJFdCmN98s",
       x = "",
       y = "Frequency") +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0,800)) +
  theme(panel.grid.major.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) +
  mapping1 +
  mapping2 +
  mapping3 +
  mapping4 +
  mapping5 +
  mapping6 +
  mapping7 +
  mapping8 +
  mapping9 +
  mapping10
```

---

class: center, middle

# [Exercise](https://jobreu.github.io/youtube-workshop-gesis-2020/exercises/B1_basic_text_analysis_exercises_question.html) time `r ji("weight_lifting_woman")``r ji("muscle")``r ji("running_man")``r ji("biking_man")`

## [Solutions](https://jobreu.github.io/youtube-workshop-gesis-2020/solutions/B1_basic_text_analysis_exercises_solution.html)