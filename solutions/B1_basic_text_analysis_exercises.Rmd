---
title: 'Basic text analysis of user comments - Solutions'
# title: 'Basic text analysis of user comments' 
author: 'Julian Kohne, Johannes Breuer, M. Rohangis Mohseni'
date: 'Automatic sampling and analysis of YouTube data, February 10-11, 2020'
output:
  unilur::tutorial_html_solution: default
  # unilur::tutorial_html: default
---

```{r knitr_init, echo=FALSE, cache=FALSE, include=FALSE}
# custom boxes
knitr::opts_template$set(clues = list(box.title = "Clues",
                                      box.body = list(fill = "#fff9dc", colour = "black"),
                                      box.header = list(fill = "#ffec8b", colour = "black"),
                                      box.icon = "fa-search",
                                      box.collapse = TRUE))

```

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy()
```

In the following exercises, we will use the data you have collected in the previous session (all comments for the video "The Census" by Last Week Tonight with John Oliver. Please note that your results might look slightly different than the output in the solutions for these exercises as we collected the comments earlier. 

You might have to adjust the following code to use the correct file path on your computer.

```{r load data}
comments <- readRDS("../data/ParsedComments.rds")
```

Next, we go through the preprocessing steps described in the slides. In a first step, we remove newline commands from the comment strings (without emojis).

```{r remove newline, message = F}
library(tidyverse)

comments <- comments %>% 
  mutate(TextEmojiDeleted = str_replace_all(TextEmojiDeleted,
                                            pattern = "\\\n",
                                            replacement = " "))
```

Next, we tokenize the comments and create a document-feature matrix from which we remove English stopwords.

```{r tokens & DFM, message = F}
library(quanteda)

toks <- comments %>% 
  pull(TextEmojiDeleted) %>% 
  char_tolower() %>% 
  tokens(remove_numbers = TRUE,
               remove_punct = TRUE,
               remove_separators = TRUE,
               remove_symbols = TRUE,
               remove_hyphens = TRUE,
               remove_url = TRUE)

comments_dfm <- dfm(toks, 
                   remove = quanteda::stopwords("english"))
```


```{block, box.title = "1", box.body = list(fill = "white"), box.icon = "fa-star"}
Which are the 20 most frequently used words in the comments on the video "The Census" by Last Week Tonight with John Oliver? Save the overall word ranking in a new object called `term_freq`.
```

```{block, opts.label = "clues"}
You can use the function `textstat_frequency()` from the `quanteda` package to answer this question.
```

```{r termfreq, solution = TRUE}
term_freq <- textstat_frequency(comments_dfm)
head(term_freq, 20)
```

```{block, box.title = "2", box.body = list(fill = "white"), box.icon = "fa-star"}
Instead of the raw frequency we can also look at the number of comments that a particular word appears in. This metric takes into account that words might be used multiple times in the same comment. What are the 10 words that are used in the highest number of comments on the video "The Census" by Last Week Tonight with John Oliver?
```

```{block, opts.label = "clues"}
You should use the variable `docfreq` from the `term_freq` object you created in the previous task.
```

```{r docfreq, solution = TRUE}
term_freq  %>% 
  arrange(-docfreq) %>% 
  head(10)
```

We also want to look at the emojis that were used in the comments on the video "The Census" by Last Week Tonight with John Oliver. Similar to what we did for the comment text without emojis, we first need to wrangle the data (remove missings, tokenize emojis, create DFM).

```{r wrangle emoji data}
emoji_toks <- comments %>% 
  mutate_at(c("Emoji"), list(~na_if(., "NA"))) %>%
  mutate (Emoji = str_trim(Emoji)) %>% 
  filter(!is.na(Emoji)) %>% 
  pull(Emoji) %>% 
  tokens()

EmojiDfm <- dfm(emoji_toks)
```

```{block, box.title = "3", box.body = list(fill = "white"), box.icon = "fa-star"}
What were the 10 most frequently used emojis comments on the video "The Census" by Last Week Tonight with John Oliver?
```

```{block, opts.label = "clues"}
The solution is essentially the same as the one for the first task in this exercise (word frequencies).
```

```{r emoji freq, solution = TRUE}
EmojiFreq <- textstat_frequency(EmojiDfm)
head(EmojiFreq, n = 10)
```

```{block, box.title = "4", box.body = list(fill = "white"), box.icon = "fa-star"}
The ranking based on raw counts of emojis might be affected by *YouTube* users "spamming" emojis in their comments (i.e., using the same emojis many times in the same comment). Hence, it makes sense to also look at the number of unique comments that an emoji appears in. What are the 10 emojis that appear in the highest number of comments on the video "The Census" by Last Week Tonight with John Oliver?
```

```{block, opts.label = "clues"}
The solution is essentially the same as the one for the second task in this exercise (docfreq for words).
```

```{r emoji docfreq, solution = TRUE}
EmojiFreq  %>% 
  arrange(-docfreq) %>% 
  head(10)
```

```{block, box.title = "Bonus", box.body = list(fill = "white"), box.icon = "fa-star"}
If you're finished with tasks 1-4 and/or want to do/try out something else, you can create an emoji plot similar to the one you saw in the lecture slides for the video "The Census" by Last Week Tonight with John Oliver. We have created a script containing a function for the emoji mapping which you can source with the following code (NB: you might have to adjust the path to the script in the code below). You might also want to have a look at the `emoji_mapping_function.R` file to see what this functions does.
```

```{r source mapping function}
source("../scripts/emoji_mapping_function.R")
```


```{block, opts.label = "clues"}
You need to add the mapping objects to your plot. To see how you can construct the plot, you can have a look at slide #25 from the session on *Basic Text Analysis of User Comments*.
```

```{r emoji plot, solution = TRUE}
create_emoji_mappings(EmojiFreq, 10)

head(EmojiFreq, n = 10) %>% 
ggplot(aes(x = reorder(feature, -frequency), y = frequency)) +
  geom_bar(stat="identity",
           color = "black",
           fill = "#FFCC4D") + 
  geom_point() +
  labs(title = "Most frequent emojis in comments",
       subtitle = "The Census: Last Week Tonight with John Oliver (HBO)
       \nhttps://www.youtube.com/watch?v=1aheRpmurAo&t=33s",
       x = "",
       y = "Frequency") +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0,150)) +
  theme(panel.grid.major.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) +
  mapping1 +
  mapping2 +
  mapping3 +
  mapping4 +
  mapping5 +
  mapping6 +
  mapping7 +
  mapping8 +
  mapping9 +
  mapping10
```